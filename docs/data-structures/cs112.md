<style>.markdown-body {font-family:"Times New Roman", Times, serif;}</style>

CS112 - Data Structures
===

- [CS112 - Data Structures](#cs112---data-structures)
	- [Review of CS111](#review-of-cs111)
		- [In the Java Virtual Machine (JVM):](#in-the-java-virtual-machine-jvm)
	- [Union Find](#union-find)
		- [Connectivity Problem:](#connectivity-problem)
			- [Algorithm 1 (Quick-find):](#algorithm-1-quick-find)
				- [Operation 1: Union.](#operation-1-union)
				- [Operation 2: Find.](#operation-2-find)
				- [Example:](#example)
			- [Quick Union - Lazy Approach:](#quick-union---lazy-approach)
			- [Weighted Quick Union](#weighted-quick-union)
				- [Analysis:](#analysis)
		- [Cost Model](#cost-model)
			- [If you ALWAYS connect the smaller tree to the bigger tree, you will always have a better running time.](#if-you-always-connect-the-smaller-tree-to-the-bigger-tree-you-will-always-have-a-better-running-time)
		- [The Problem with Arrays](#the-problem-with-arrays)
			- [Instead, we can use:](#instead-we-can-use)
	- [Linked Lists](#linked-lists)
		- [So, what is a Node?](#so-what-is-a-node)
	- [Recitation #1](#recitation-1)
	- [Stacks and Queues](#stacks-and-queues)
			- [Fundamental data types](#fundamental-data-types)
		- [Stack](#stack)
			- [Parameterized Stacks:](#parameterized-stacks)
			- [Autoboxing:](#autoboxing)
		- [Queue](#queue)
	- [Recitation #1](#recitation-1-1)
		- [Building a Decision Tree](#building-a-decision-tree)
			- [Worst Case Comparisons:](#worst-case-comparisons)
		- [Binary Search Trees](#binary-search-trees)
	- [Symbol Table](#symbol-table)
		- [ArrayList\<key, value\>](#arraylistkey-value)
	- [Binary Search Tree (BST)](#binary-search-tree-bst)
			- [The Best case:](#the-best-case)
			- [The Worst Case:](#the-worst-case)
		- [Ways to Traverse a BST](#ways-to-traverse-a-bst)
		- [2-3 Trees](#2-3-trees)
			- [When you insert into a 3-node:](#when-you-insert-into-a-3-node)
		- [Red-black BSTs](#red-black-bsts)
			- [Left Leaning Red-Black Trees](#left-leaning-red-black-trees)
			- [Left Leaning Red-black trees are EQUIVALENT to a BST such that:](#left-leaning-red-black-trees-are-equivalent-to-a-bst-such-that)
			- [Left leaning Red-black Tree Inserting](#left-leaning-red-black-tree-inserting)
		- [Really fucking important: 2 red links = a 4-node!!!](#really-fucking-important-2-red-links--a-4-node)
			- [There are 3 Different operations in order to Sort out a LLRB BST:](#there-are-3-different-operations-in-order-to-sort-out-a-llrb-bst)
			- [Elementary red-black BST operations](#elementary-red-black-bst-operations)
		- [Collections](#collections)
			- [A Complete Binary Tree](#a-complete-binary-tree)
			- [Binary Heap](#binary-heap)
			- [To Insert:](#to-insert)
			- [To Build:](#to-build)
			- [To Delete Max:](#to-delete-max)
		- [Heapsort](#heapsort)
			- [Heapify](#heapify)
			- [Sortdown](#sortdown)
	- [Hash Tables](#hash-tables)
			- [Overview:](#overview)
			- [General Idea:](#general-idea)
		- [Hashing: Basic Plan](#hashing-basic-plan)
			- [Issues:](#issues)
		- [Hash Function](#hash-function)
			- [Computing the Hash Function](#computing-the-hash-function)
			- [Uniform Hashing Assumption](#uniform-hashing-assumption)
		- [Separate-chaining Symbol Table](#separate-chaining-symbol-table)
			- [Separate Chaining: Insertion](#separate-chaining-insertion)
			- [Rehashing in a Separate Chaining Hash Table](#rehashing-in-a-separate-chaining-hash-table)
			- [Deletion in a Separate Chaining Hash Table](#deletion-in-a-separate-chaining-hash-table)
			- [Overview of Separate Chaining](#overview-of-separate-chaining)
		- [Collision Resolution: Open Addressing](#collision-resolution-open-addressing)
			- [Resizing Linear Probing Hash Tables](#resizing-linear-probing-hash-tables)
	- [Undirected Graphs](#undirected-graphs)
		- [Why study graph algorithms?](#why-study-graph-algorithms)
			- [Social Networks](#social-networks)
		- [Terminology](#terminology)
		- [Graph Representation: Adjacency Martrices](#graph-representation-adjacency-martrices)
			- [*Alternative*:](#alternative)
			- [The Problem With Adjacency Matrices](#the-problem-with-adjacency-matrices)
		- [Graph Representation: Adjacency Lists](#graph-representation-adjacency-lists)
			- [*Alternative:*](#alternative-1)
		- [Why Adjacency Lists?](#why-adjacency-lists)
		- [However, if the graph is extremely dense...](#however-if-the-graph-is-extremely-dense)
		- [Space Consumptions:](#space-consumptions)
		- [Summary](#summary)
			- [Question:](#question)
	- [Undirected Graphs](#undirected-graphs-1)
			- [Tremaux maze exploration](#tremaux-maze-exploration)
		- [**Depth-first Search**:](#depth-first-search)
	- [Directed Graphs](#directed-graphs)
		- [Digraph API](#digraph-api)
		- [Digraph: Adjacency Matrix](#digraph-adjacency-matrix)
		- [Digraph: Adjacency Lists](#digraph-adjacency-lists)
	- [Depth-first search](#depth-first-search-1)
		- [Reachability](#reachability)
			- [In Digraphs:](#in-digraphs)
		- [In Code:](#in-code)
		- [Breadth-first search in Digraphs](#breadth-first-search-in-digraphs)
		- [Topological Sort](#topological-sort)
		- [Insertion Sort](#insertion-sort)
		- [Selection Sort](#selection-sort)
		- [Merge Sort](#merge-sort)
		- [Quicksort](#quicksort)


Review of CS111
---

### In the Java Virtual Machine (JVM):

 Stack:
- The stack holds the primitive data type directly in the stack.


	- They are "stacked" in the stack.
	- If they are not a primitive data type, then the stack holds the location of the variable in the "HEAP".
	- The size of items in the stack can  NOT change.

Heap:

- The heap is basically just a jumbo of data with different variables in the stack pointing to the heap.
- The abstract data types are stored in the heap.

Union Find
---

Today, we will see 3 algorithms that have different runtimes.

### Connectivity Problem:
1. m vertices
2. 2 operations:
	- Connect two vertices
	- Is there a path between the two vertices?

#### Algorithm 1 (Quick-find):
- The data structure used to implement this will be a array.
- the nodes p and q are connected if and only if they have the same id.
##### Operation 1: Union.

----

To merge components containing p and q, change all entries whose id equals id[p] to id[q].

----

- Start off with an array. Each number is a vertice.

| Vertice     | 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   |
| ----------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Data Stored | 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   |

1. Connect (3,4), the value under 3 gets replaced by the 4.

| Vertice     | 0   | 1   | 2   | 3       | 4   | 5   | 6   | 7   | 8   | 9   |
| ----------- | --- | --- | --- | ------- | --- | --- | --- | --- | --- | --- |
| Data Stored | 0   | 1   | 2   | ***4*** | 4   | 5   | 6   | 7   | 8   | 9   |

2. Connect (2,8), the value under 2 gets replaced by the 8.

| Vertice     | 0   | 1   | 2       | 3   | 4   | 5   | 6   | 7   | 8   | 9   |
| ----------- | --- | --- | ------- | --- | --- | --- | --- | --- | --- | --- |
| Data Stored | 0   | 1   | ***8*** | 4   | 4   | 5   | 6   | 7   | 8   | 9   |


3. Now, if I merge (4, 9), then then **all values** of 4 in the array gets changed.

| Vertice     | 0   | 1   | 2   | 3       | 4       | 5   | 6   | 7   | 8   | 9   |
| ----------- | --- | --- | --- | ------- | ------- | --- | --- | --- | --- | --- |
| Data Stored | 0   | 1   | 8   | ***9*** | ***9*** | 5   | 6   | 7   | 8   | 9   |

##### Operation 2: Find.
- First, you look up the first value. (Ex: Does 4 connect to 9?)
- The value stored at 4 is 9.
- Next, you find the value at 9, which is still 9.
- Compare the two, and you will find that they **do** connect.

---

For the second operation, we want it to be faster.

Instead, we don't store the id of the vertice in the array.

**A parent is the one we designate as the representative of the family.**

##### Example:

| Vertice     | 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   |
| ----------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Data Stored | 0   | 1   | 9   | 4   | 9   | 6   | 6   | 7   | 8   | 9   |

Parents:

	0 - is 0's parent
	1 - is 1's parent
	3 - is the child of 4
	4 - is the child of 9
	5 - is the child of 6
	6 - the parent of 5
	7 - is 7's parent
	8 - is 8's parent
	9 - is it's own parent

#### Quick Union - Lazy Approach:
- Uses a integer array id[] of size N.
- So a better way to think of it is:
	- **Root** of i is id[id[...id[i]...]].
- Lookup occurs by finding the root parent of the number.
- Example: To find whether 2 and 3 are connected, look for the root of 2 first.
	- The root of 2 is 9, 9 is the parent.
  - Look for the root of 3. 3 points to 4, which then points to 9, making the root of 3 be 9.
  - Therefore, 2 is connected to 3.
- ***In layman's terms***: Instead of replacing the ids with the same ones, we just attach it without changing it.


#### Weighted Quick Union
- Modify quick-union to avoid tall trees.
- Keep track of the size of each tree (number of elements)
- Always link root of smaller tree to root of larger tree.

##### Analysis:
- Depth of any node (*x*) is *log n*. (log base 2)
	- Everytime you connect two trees, you will have one tree point to another tree. 

### Cost Model

| algorithm            | initialize | union   | find    |
| -------------------- | ---------- | ------- | ------- |
| quick find           | n          | n       | 1       |
| quick union          | n          | n       | n       |
| weighted quick union | n          | *log n* | *log n* |

#### If you ALWAYS connect the smaller tree to the bigger tree, you will always have a better running time.


### The Problem with Arrays
- When your array is full and you want to make a new array, to copy things over to the new array, the array takes O(n) time to copy everything over.

#### Instead, we can use:

## Linked Lists
- Empty
- Nodes linked together

### So, what is a Node?
- A node has 2 pieces.
	- It has the data piece, and the link part, which will allow us to link to the next.


```Java

public class Node {
	int data; // This will be the data stored.
	Node next; // This will point to the next node.
	
	Node() {
		data = 0;
		next = null;
		}

	public static void main(String[] args) {
		Node f = new Node(); // Here the data is (0, null)
		f.data = 31;	// The data now becomes (31, null)
		Node s = new Node(); // Here the data is (0, null)
		s.data = 8; // s = (8, null)
		f.next = s;	// Use this to update the address containted in variable f.
		// f now becomes (31, s)
		}
	}

```

## Recitation #1

**Tilde Notation**: Basically Big O except keeping coefficient.

## Stacks and Queues

#### Fundamental data types
- Value: Collection of objects
- Operations: add, remove, iterate, test if empty
- Intent is clear when we add

### Stack
- It's like a stack of books.
- Each variable gets stacked on top of each other.
- Adding to the top of the stack is called "push"
- To remove an item from the top of the stack is called "pop"

**Overflow**: When you try to add an item into an stack with no space

**Underflow** : When you try to take an object out of a stack with nothing in it.

What happens if the array overflows? You must resize:

**Increasing array for a stack:**

There are two ways you can go about it.

1) Increase it by one.
	- Which means each time you push a new item, you'd have to make a new array and copy everything over. This is terrible and ends up being n time for each item leading to quadratic time.
2) Double it.
	- By doubling it, you're essentially paying for more so that you dont have to keep adding. Much better than incrementing by one.

**Decreasing array**:
- You should only decrease the array when the array is one-quarter full.

**So!!**

The most efficient way for stacks is:

- push() double size of array s[] when array is full.
- pop() halve size of the array s[] when the array is a quarter full.

**Stack implementation: resizing arrays vs Linked List**:

Linked list implementation:
- Every operation takes constant time in the worst case.
- Uses extra time and space to deal with the links.

Resizing-array implementation:
- Every operation takes constant amortized(buying more so u got leftover space) time.
- Less wasted space


#### Parameterized Stacks:
- Add a type parameter in the stack initialization so that it only accepts a certain type. This allows us to get a compile time error instead of a runtime error.
- These are called generic types, which is better than strings.
- Helps handle multiple types of data.
- Parameterized stacks only work with linked lists. The arrays in Java do not allow for generic types to initialize arrays.

#### Autoboxing:
- Java helps you deal with primitive types.

Wrapper style:
- Each primitive type has a **wrapper** object type.
- Ex: *Integer* is the wrapper type for *int*.

**Autoboxing**: Automatic cast between a primitive type and its wrapper.


### Queue
- Many Different types of queues.
- Last in First Out (LIFO), the oldest item in the queue will be the first one out.
- First in First Out (FIFO), the newest item in the queue will be the first one out.
- Removing an item from the queue is called **dequeue**,, inserting is called **enqueue**.


## Recitation #1
- Quick Find - Time Complexity is O(1)
- Quick Union - Time Complexity is O(n)


### Building a Decision Tree

- Decision tree is a theoretical tool used to analyze the running time of algorithms. It illustrates the possible execturions on inputs.
- You have to build the tree by running the algorithm.
	- So for each item, you start building that item.
	- The middle point is the middle point and you start building off of that.

Here is an Example:

<br>

![Decision Tree](imgs/decision-tree.png)

#### Worst Case Comparisons:


Worst Case for successful Search:
- Average the number of comparisons for successful searches.

| Index   | # of comparisons |
| ------- | ---------------- |
| 5       | 1                |
| 2       | 2                |
| 8       | 2                |
| 1       | 3                |
| 4       | 3                |
| 7       | 3                |
| 9       | 3                |
| 0       | 4                |
| 3       | 4                |
| 6       | 4                |
| ------- | 29/10 = 2.9      |

Worst Case for Unsuccessful Search:
- If we don't know how many keys are being searched, we can only affirm that the average will be between 3-4.

### Binary Search Trees

- At this point, we know different data structures, arrays and linked lists.

Why are arrays good:

	- Arrays are fast.
	- Array reads occur in O(1) time.
	- If you have a index for something in the array, reading it occurs in constant time.

Why are Linked lists good:

	- Linked lists can add unlimited items in O(n) time as long as you can find the end.
	- Especially using binary search on a linked list.
         - Linked lists that implement binary search run in O(n log n) time. 


## Symbol Table

### ArrayList<key, value>
- Stores a key/value pair
	- In a ArrayList, the first key is the index in the array.
	- The second key is the value.
- However, ArrayLists are slow, very very slow. The worst case for finding something in a 2D array would be O(n^2).
- The running time to add to a ArrayList is linear, or O(n).
	- That is slowwww.

Symbol Lists in general uses the same <key, value> pairing that exists in ArrayLists. 

However, usually, the key is not the "index of the array" but something unique to identify you...

**Store** <key, value>

A Symbol Table API:

```

public class ST<key extends Comparable<key>, Value>

	ST()		// create an empty symbol table

	void put(Key key, Value val)	// insert key-value pair

	Value get(Key key) 	// value paired with key

	boolean contains(Key key)	// is there a value paired with key?
	
	Iterable<Key> keys()	//all the keys in the symbol table.

	void delete(Key key)

	boolean isEmpty()

	int size()

```

## Binary Search Tree (BST)

![Binary Search Tree](imgs/bst.png)

In a Binary Search Tree, each node will have to keep track of the:
- key
- value
- left node
- right node

#### The Best case:
- When you have a very full tree where the height of the tree is log(n), then that is also the amount of time it would take to find something.
- O(log n) for insert
- O(log n) for search

#### The Worst Case:
- When you have a very skewed list, the length of time to find something is O(n) time.
- This means that it take O(n) time for both insert and search.
- This is fixable!!

In order to Insert:

Step 1: Search until it fails.
Step 2: Insert at failure point.

| 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 5   | 5   | 3   | 3   | 4   | 5   | 6   | 5   | 3   | 9   |

| 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 7   | 5   | 8   | 3   | 4   | 2   | 6   | 1   | 8   | 9   |


### Ways to Traverse a BST

1. In order traversal
- Outputs the data in order.

---
To find the review: Click [here](https://ivanz505.github.io/runb-notes/data-structures/midterm1)

---

### 2-3 Trees

- The problem with BSTs is that it could take UP TO linear time to delete and search.
- We need a data structure that GUARANTEES logarithmic time.
- We call these balanced trees.
- The 2-3 Tree has 2 types of nodes:
	- A 2 node, where there is a node has one key and 2 children.
	- A 3 node, where there is a node has two keys and 3 children.
		- In a 3 node, the left leaf contains all numbers smaller than the first key.
		- The middle leaf contains all the values between the first and second key.
		- The right leaf contains values greater than the second key.
- 2-3 Trees maintain a symmetrical order.
- They maintain a perfect balance, meaning that one side will not get significantly longer than the other.

![Two Three Tree](imgs/twothreetree.png)

#### When you insert into a 3-node:
- You first turn turn the 3-node into a 4-node **temporarily**.
- This means that the 3-node now has 3 keys, making it a 4-node.
- Absolutely not acceptable so what you do is you pass the middle key up to the parent branch.
- If the parent is a 3-node, you keep passing it up the tree.
	- If not a 3-node and is a 2-node, you can just insert it and make the 2-node a 3-node.
- If the 3-node is the root of the tree, you pass the middle key up and make it the new root by making a new node! (**Only time you make a new node!!**)

### Red-black BSTs

#### Left Leaning Red-Black Trees
- Now, the problem with the 2-3 tree is that it is difficult to implement!
- To make it simple, they thought, how can they represent a 3-node with a 2-node?
- In order to accomplish this, you split the 3-node into 2 different nodes that are linked together with a <b style="color: red">red</b>.
- The larger key becomes the top item of the red-black tree and the smaller key gets connected to the LEFT with a <b style="color: red">red</b> link.

![Red black vs 2-3](imgs/redblackv23.png)

What's important to realize for the height is:
- In a 2-3 Tree, the height is of at most log_2(n). In best case, log_3(n).
- In the Red-Black tree, you need to think about the red links as **horizontal** meaning that the height would still be of log_2(n) at worst case
- But even if you didn't do that and counted the red links:

#### Left Leaning Red-black trees are EQUIVALENT to a BST such that:
1) No nodes have 2 red links connected to it. (Left leaning)
2) Every path from root to null has the same number of blacck links
	- This means there is perfect black balance.
3) All the red links lean left.

#### Left leaning Red-black Tree Inserting
- In order to insert into a 2-node, you need to:
1) Do a BST insert
2) Color the new link Red
3) Make sure that you follow all of the equivalence statements above.
4) If the parent realizes it has 2 red links, then it flips the colors of its 2 children links and flips itself. See image. Then it changes it's link to it's own parent to the opposite color.
5) Then, the parent's parent (grandparent) realizes there's a problem and flips it again.

### <b style="color: green">Really fucking important: 2 red links = a 4-node!!!</b>

#### There are 3 Different operations in order to Sort out a LLRB BST:

- Flip the colors of the links of a parent.
- Rotate right
- Rotate left

#### Elementary red-black BST operations

- **Left rotation**: Orient a (temporarily) right leaning red link to lean left.

TODO: Find image

- **Right rotation**: Orient a left leaning red link (temporarily) right.

### Collections
- A **collection** is a data type that stores a group of items.

| data type      | core operations            | data structures    |
| -------------- | -------------------------- | ------------------ |
| stack          | push, pop, peek            | linked list, array |
| queue          | enqueue, dequeue, peek     | linked list, array |
| priority queue | insert, remove, peek       | binary heap, BST   |
| symbol table   | put, get, contains, delete | BST, hash table    |
| set            | add, remove, contains      | BST, hash table    |

The challenge is to find the right data structure for the right job and efficiently implement it.

| Implementation  | INSERT | DELETE-MAX | MAX   |
| --------------- | ------ | ---------- | ----- |
| unordered array | 1      | N          | N     |
| ordered array   | N      | 1          | 1     |
| goal            | log N  | log N      | log N |

Solution: "locally" ordered array, a.k.a. binary heap.

#### A Complete Binary Tree

Binary tree: empty or node with links to left and right binary trees.

Complete binary tree: Every level (except possibly the last) is completely filled, and all nodes are as far left as possible.

Properties:
- Height of a complete binary tree of size N is log N.
- Height increases only when n is a power of 2.
- Number of nodes in a complete binary tree of height h is 2^h - 1.

#### Binary Heap
- Array representation of a heap-ordered complete binary tree.

Heap-ordered binary tree: 
- Keys in nodes
- Parent's key no smaller than children's keys.

Array Reprensentation:
- Indices start at 1.
- Takes nodes in level order.
- No explicit links .
needed

Why does an array for max priority queue not start at 0?
- Because then the parent of a node at index k would be at index (k-1)/2, which is not an integer.
- Or, you can look at it this way, if you start at k=0, the left index would be 2k, and the right index would be 2k+1. This is not possible!!!

#### To Insert:

![Heap Insert](imgs/heap-inserting.gif)

#### To Build:

![Heap Build](imgs/heap-building.gif)

#### To Delete Max:

![Heap Delete Max](imgs/heap-deleting.gif)

### Heapsort
- Heapsort is divided into 2 parts
- The first part is heap construction (Heapify)
- The second part is sortdown
- Heapsort runs in O(`n log n`) time, but is actually `n + n log n`, making it slower.
- The best case is O(n).
- This is a inplace algorithm, which means it doesn't use too much more memory.
- It is not stable. (WIt is not stab (Which means order of duplicate keys are changed)hich means order of duplicate keys are changed)

#### Heapify
- In heapify, you start from the item in the array that is not a leaf node and sink everthing from `n/2` to the first item in the array.
- Then, the array would be a heap.

#### Sortdown
- In sortdown, you start from the very end of the array and swap it with the parent and then sink the new parent.
- Repeat with every single item in the array.
- Then you have a sorted min/max heap.



## Hash Tables

#### Overview:
- Hash functions
- separate chaining
- linear probing
- context

Hash Tables attempt to insert, delete, and search for items in constant time.

Hash Tables use arrays and indices but uses something called a hash function to determine the index.

#### General Idea:
- If keys are small integers, we can use an array to implement a symbol table.
- Intepret key as an array index and store the value associated with key i in array position i.
- The issue is that you would need a table of size 2^32 to store 4-byte integers (too large)

### Hashing: Basic Plan
- Save items in a **key-indexed table** (index is a function of the keys).
- **Hash function**: Method for computing array index from key.

#### Issues:
- Computing the hash function.
  - The hash function has to be FAST! So, it can not take too long.
- Equality test: Method for checking whether two keys are equal.
- Collision resolution: Algorithm and data structure to hande two keys that hash to the same array index.
  - The two techniques:
    - Linear probing: If the index is taken, then you go to the next index and check if it's taken. If it is, then you go to the next index and so on.
    - Separate chaining: If the index is taken, then you make a linked list at that index and add the new key to the linked list.

### Hash Function

---

- **Hash function**: Maps keys to integers in a fixed range.

- `hash(key) = (key) % M`

#### Computing the Hash Function

**Idealistic goal**: Scramble the keys uniformly to produce a table index.

- It should be deterministic -- same key always maps to same index.
- Efficiently computable
- Each table index should be equally likely for each key!!

#### Uniform Hashing Assumption
- Each key is equally likely to hash to an integer between 0 and M-1.

Example: Throwing balls uniformly at random into m bins. Should be an equal chance to land in any of the bins.

### Separate-chaining Symbol Table

---

Use an array of linked lists to implement a symbol table.
- Hash: map key to integer i between 0 and M-1.
- Insert: put at front of ith chain if not already in chain.
- Search: Sequential search in ith chain.

![Separate Chaining](imgs/separate-chaining.png)

#### Separate Chaining: Insertion
- Insertion is easy. Just add the new key to the front of the linked list.
- m = 2, `hash(key) = (key) % M`
- We want the strings to be as short as possible so that the search time is as fast as possible.

#### Rehashing in a Separate Chaining Hash Table

Goal: Average length of list *n / m* is constant.
- Double table size when average length of list *n / m* exceeds a constant *t* usually 8.
- Halve table size when average length of list *n / m* is less than a constant *t* usually 2.
- Note: Need to rehash all keys when resizing.

![Rehashing](imgs/rehashing-separate-chaining.png)

#### Deletion in a Separate Chaining Hash Table
- Need to only consider the chain containing key.
- Use the hash function to find the array indices that the key would be in then find it and delete from the linked list.

#### Overview of Separate Chaining

![Separate Chaining Summary](imgs/separate-chaining-summary.png)

### Collision Resolution: Open Addressing

---

- Maintain key and value in two parallel arrays.
- When a new key collides, find next empty slot, and put it there.
- Also called linear-probing.
- The displaecment of the key from where it should be placed could end up being a long distance, making it size *n*.

![Linear Probing](imgs/linear-probing.png)

#### Resizing Linear Probing Hash Tables

Goal: Average length of list *n / m* is less than 1/2.
- Double array size (*m*) when average length of list *n / m* is greater than or equal to 1/2.
- Halve array size (*m*) when average length of list *n / m* is less than or equal to 1/8.
- Need to rehash all keys when resizing.

TODO: Get notes from 4/4 lecture.

<br>

## Undirected Graphs

---

- **Graph**: Set of *vertices* connected pairwise by *edge*.

### Why study graph algorithms?
- Thousands of practical applications.
- Hundreds of graph algorithms known.
- Interesting and broadly useful abstraction.
- Challenging branch of computer science and discrete math.

#### Social Networks
- A vertex is a person
- An edge is a social relationship

### Terminology
- Graph: Refer up
- Path: Sequence of vertices connected by edges.
- Connected: There exists an edge between the two vertices.
- Cycle: Path whose first and last vertices are the same.

### Graph Representation: Adjacency Martrices
- We use a **symbol table** in order to map the vertices and the edges.
- Maintain a two-dimensional V-by-V boolean array.
- For each edge v-w in graph: `adj[v][w] = adj[w][v] = true`
  - This is because, if theres a edge for v to w, there will be one from w to v. It's just how a line works lol.

---
#### *Alternative*:
Instead of boolean matrix, make it an integer matrix and add the edge cost or value as a number for adjacency matrices.

---

![Adjacency Matrix](imgs/adjacency-matrix.png)

---
What does the graph API have?

```Java
public class Graph {
	Graph(int v)		// create an empty graph with V Vertices.
	void addEdge(int v, int w)	// add an edge between v and w.
	Iterable<Integer> adj(int v)	// shows vertices adjacent to v.
	int V()		// number of vertices
	int E()		// number of edges
}
```

--- 

What is the runtime of the code fragment?

```Java
for(int v=0; v < G.V(); v++) {
	for(int w: G.adj(v)) {
		StdOut.println(v + "-" + "w");
	}
}
```

The runtime is `V^2`. 

#### The Problem With Adjacency Matrices
- If the graph is extremely sparse, the matrices will be extremely empty.
- Waste of a lot of space.
- Instead...

### Graph Representation: Adjacency Lists
- Maintain a vertex-indexed array of lists.
- Each array index is the start of a linked list.
- The linked list itself will contain all the neighbors of that vertex.

---
#### *Alternative:*
Add the edge weight to the node as another data value in the node.

---

![Adjacency List](imgs/adjacency-list.png)

### Why Adjacency Lists?
- Adjacency lists can return the neighbors of one vertex extremely rapidly, as compared to Adjacency Matrix.
- It has a **worst-case** scenario of `O(E)` to return the number of neighbors where E is the number of edges in the vertex.

### However, if the graph is extremely dense...
- It does not matter which one we use, as the lists would all be the same size.

### Space Consumptions:
- It takes V * (2E) *2
- `2E` because there are 2 units per node: the vertex and the next pointer.
- Then, multiplied by 2 because each node appears twice.

**IN PRACTICE** we use adjacency-lists representation.
- Algorithms based on iterating over vertices adjacent to *v*.
- Real-world graphs tend to be sparse (not dense)

---
### Summary
![Matrix v List](imgs/adjacent-matrix-v-list.png)

---

#### Question:
What is the maximum number of edges on a graph of V vertices?

Ans: V * (V - 1)/2

Reason: You would technically be able to connect V to every single other vertice other than itself, namely `V-1`. However, where there's a edge connecting v to w, there would be one connecting w to v, meaning you need to divide that entire number by 2.

---

## Undirected Graphs
---

Maze Exploration:
- Maze as a graph.
- The intersections are the vertices.
- The passages are the edges.

#### Tremaux maze exploration
- Unroll a ball of string behind you
- Mark each newly discovered intersection and passafe.
- Retrace the steps when there are no more unmarked options.

### **Depth-first Search**:
---

Goal: Systematically traverse a graph.
Idea: Mimic maze exploration.

**DFS** should be done recursively.

Steps:
1. Call DFS on a vertex.
2. Mark that vertex as explored.
3. For every unmarked edge connected to that vertex, mark the path that got you to this new vertex connected to the original vertex. Call DFS again. (**DEPTH-FIRST**- IT GOES DEEP)

## Directed Graphs

- Digraph API
- Depth-first search
- breadth-first search
- topological search

---

**Directed graphs** - Set of vertices connected pairwise by **directed** edges.

- Each vertex has an indegree and an outdegree. (How many edges connect to it and how many does the vertex extend out.)

---

### Digraph API

```Java

public class Digraph {

	Digraph(int V)		// create an empty digraph with V vertices.
	void addEdge(int v, int w) 	// add a directed edge v-> w
	Iterable<Integer> adj(int v)	// vertices adjacent from v
	int V() 		// number of vertices
	int E() 		// number of edges
	Digraph reverse() 	// reverse this digraph
	String toString() 	// String representation
}

```

---

### Digraph: Adjacency Matrix

- With digraphs using adjacency matrices, the `adj[v][w] != adj[w][v]`, because the connection is only one way!

![Digraph adjacency matrix](imgs/digraph-adjacency.png)

### Digraph: Adjacency Lists

- The number of nodes in this list is going to be the number of edges that  are in the graph. (The nodes are the number of out edges)
- All the outdegrees will add together to be the # of total nodes in the adjacency list.

![Adjacency Lists](imgs/digraph-list.png)

---

Digraphs Quiz:

1. Which is order of growth of rinning  time of removing an edge v→w from a digraph uses the adjacency-lists representation,where V is the number of vertices and E is the number of edges?

	**Ans**: outdegree(v), because if you want to remove an edge, the worst case number of traversals you would need to find that edge would be the outdegree, or the # of nodes in the linked list of the index of where v is linked to w.

2. What is the maximum number of edges on a digraph of V vertices?

	**Ans**: V * (V - 1) because each vertex can have at most V-1 number of edges going from it, cus it can't have an edge going to itself. So, if every vertex has the max number of edges away, then you would have V * (V - 1).

---

| Representation   | Space | Insert edge v->w | edge from v to w | iterate over vertices |
|------------------|-------|-------------------------|------------------|-----------------------|
| list of edges    | E     | 1                       | E                | E                     |
| adjacency matrix | V^2   | 1                       | 1                | V                     |
| adjacency lists  | E + V | 1                       | outdegree(v)     | outdegree(v)          |

## Depth-first search

### Reachability
- Problem: Find all vertices reachable from *s* along a directed path.

#### In Digraphs:
- Smae method as undirected.
- Every unidrected graph is a digraph (with edges in both directions)
- DFS is a digraph algorithm.

![DFS Digraph](imgs/dfs-digraph.png)

### In Code:

- Maintain a boolean array marked if visited.
- Your constructor for DirectedDFS marks this.
- Run a recursive DFS method which marks the vertex you're on as visited, and then runs the method again on all the adjacent edges to the vertex you're on.
- Repeat.

**The number of checks is = to the number of edges**

### Breadth-first search in Digraphs

- BFS puts a vertex *s* into a FIFO queue, and mark s as visited.
- Then, repeat until the queue is empty:
	- Remove the least recently added vertex *v*.
	- for each unmarked vertex adjacent to *v*:
		- add to queue and mark as visited.

---

### Topological Sort


### Insertion Sort
- Compare with previous.
- Swap if needed.
- Best case: O(n), worst case: O(n^2)

### Selection Sort
- Finds smallest items in array and then swap.
- Best case: O(1), worst case: O(n^2)

### Merge Sort
- Breaks down a rray into arrays of size one, then merges the two arrays and sorts them together.
- Best and worst case: O(n log n)

### Quicksort

- Quicksort identifies the point (partition point) in the array where everything to the left is **smaller** and everything to the right is **greater**. 
- Then, you keep recursively dividing up the array into different partition points on the left and right.

```

quicksort(int[] a, int lo, int hi) {

	if(hi <= lo) return;
	int p = partitition(a, lo, hi);  // Everything smaller than the item at the partition point will be to the left. This would mean the item at a[p] would be in the right position.
	quicksort(a, lo, p-1);   // recursive
	quicksort(a, p+1, hi);

}

partition(int[] a, int lo, int hi) {

	i = low+1, j = hi

	REPEAT UNTIL i and  j pointers cross
		scan from left to right as long as a[i] < a[lo]
		scan from right to left as long as a[j] > a[lo]
		SWAP a[i]with a[lo]
		
	WHEN pointers CROSSED
		SWAP a[j] with a[lo]
		RETURN j
}		

```



